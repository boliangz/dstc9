# -*- coding: utf-8 -*-
"""
Created on Mon Mar 23 21:03:36 2020

@author: truthless
"""
import os
import random
import torch
import re
import collections
import traceback
import torch.distributed as dist
import netifaces as ni
import spacy
from unidecode import unidecode
from fuzzywuzzy import fuzz

from convlab2.dialog_agent import Agent

from end2end.soloist.methods.baseline.dataset.dataset import SoloistDataset
from end2end.soloist.methods.baseline.nn.gpt import GPT2Model
from end2end.kb_search import get_db_state
from end2end.post_processing import get_final_response, post_process_constraints
from end2end.clean_dataset import clean_text


DEFAULT_DIRECTORY = os.path.join(
  os.path.dirname(os.path.abspath(__file__)),
  os.pardir,
  os.pardir,
  # 'experiment_management/model/baseline_dialog_act',
  # 'experiment_management/model/baseline_entrance_fee/',
  'experiment_management/model/baseline_includeDuplicatePlaceholder/'
  # 'experiment_management/model/{}'.format(
  #   os.path.basename(os.path.dirname(os.path.abspath(__file__)))
  # )
)
DEFAULT_ARCHIVE_FILE_URL = "https://convlab.blob.core.windows.net/convlab-2/damd_multiwoz_data.zip"
DEFAULT_MODEL_URL = "https://convlab.blob.core.windows.net/convlab-2/damd_multiwoz.zip"


class Soloist(Agent):
  def __init__(self,
               model_file=DEFAULT_DIRECTORY,
               name='SOLOIST',
               max_resp_token=1024,
               user_simulator=True):
    """
    Sequicity initialization

    Args:
        model_file (str):
            trained model path or url.
    Example:
        damd = Damd()
    """
    super(Soloist, self).__init__(name=name)
    model_path = model_file

    self.max_turn = None
    self.model = self.load_model(model_path)
    self.dialog_history = []
    self.max_resp_token = max_resp_token
    self.user_simulator = user_simulator

    self.init_session()

  def load_model(self, path=None):
    # load tokenizer
    tokenizer = SoloistDataset.load_tokenizer(path)

    # load model
    print('loading model from: %s' % path)
    model_path = os.path.join(path, 'best_val_model.pth.tar')
    if not torch.cuda.is_available():
      state = torch.load(model_path, map_location=torch.device('cpu'))
    else:
      state = torch.load(model_path)

    # get model parameters
    state_parameters = state['parameters']
    state_parameters['pretrained_gpt'] = path
    parameters = state_parameters
    self.max_turn = int(parameters['max_turn'])

    # init process group
    parameters['node_num'] = 1
    parameters['rank'] = 0

    # setup distributed learning env
    parameters['world_size'] = parameters['node_num'] * parameters['gpu_num']
    os.environ['NCCL_DEBUG'] = 'INFO'
    for net_interface in ni.interfaces():
      try:
        os.environ['NCCL_SOCKET_IFNAME'] = net_interface
        self_addr = ni.ifaddresses(net_interface)[ni.AF_INET][0]['addr']
        print('net interface is set to {}'.format(net_interface))
        break
      except:
        continue
    os.environ['MASTER_ADDR'] = self_addr
    port = random.randint(3300, 61000)
    os.environ['MASTER_PORT'] = str(port)

    print('initializing process group...')
    if dist.is_available():
      dist.init_process_group(backend='nccl',
                              init_method='env://',
                              world_size=1,
                              rank=0)
      torch.manual_seed(0)

    # init model
    print('model parameters:')
    for k, v in parameters.items():
      print('  {}={}'.format(k, v))

    model = GPT2Model(**parameters,
                      model_dir=path,
                      process_idx=0,
                      replica_rank=0,
                      is_test=True,
                      tokenizer=tokenizer)

    # load pre-trained weigths
    if not torch.cuda.is_available():
      new_state_dict = collections.OrderedDict()
      for k, v in state['state_dict'].items():
        name = k[7:]  # remove `module.`
        new_state_dict[name] = v
      state['state_dict'] = new_state_dict

    model.nets.load_state_dict(state['state_dict'])
    model.nets.train(False)

    return model

  def init_session(self):
    """Reset dialog history."""
    self.dialog_history = []
    print('=' * 80)

  def response(self, usr, agent_state=None):
    """
    Generate agent response given user input.

    Args:
        observation (str):
            The input to the agent.
    Returns:
        response (str):
            The response generated by the agent.
    """
    # print('-------------------- turn starts ---------------------')

    # if restart conversation triggered
    if usr == 'restart':
      if agent_state is None:
        self.init_session()
      else:
        agent_state = []
      response = human_eval_response = 'what can i do for you today?'
    else:
      try:
        # clean user utterance
        usr = clean_text(usr)

        # add user utterance to dialog history
        if agent_state is None:
          self.dialog_history.append(usr)
          dialog_hisotry = self.dialog_history
        else:
          agent_state.append(usr)
          dialog_hisotry = agent_state
        # print('dialog_history:', dialog_history)

        # generate response
        response, human_eval_response = self._response(dialog_hisotry)

        # if response is similar to the previous system utterance, re-generate
        # the response
        response, human_eval_response = self.re_generate_similar_response(
          dialog_hisotry, response, human_eval_response
        )

        # only add the original response to the dialog history
        if agent_state is None:
          # append response to dialog history
          self.dialog_history.append(response)
        else:
          agent_state.append(response)

      # handle unkown global exception
      except Exception as e:
        traceback.print_exc()
        response = human_eval_response = \
          "i'm really sorry but i couldn't understand what you just said. " \
          "could you please provide more specific information ? Or please " \
          "type \"restart\" to restart the conversation. "

    if self.user_simulator is True:
      # for user simulator, return the original response
      return response
    else:
      # for human eval, return the customized human eval response
      return human_eval_response, agent_state

  def re_generate_similar_response(self,
                                   agent_state,
                                   response,
                                   human_eval_response):
    # if response is similar to the previous system utterance, re-generate
    # the response
    last_sys_response = None
    if agent_state and len(agent_state) >= 3:
      last_sys_response = agent_state[-2]
    elif len(self.dialog_history) >= 3:
      last_sys_response = self.dialog_history[-2]

    if last_sys_response:
      beam_size = 2
      while fuzz.partial_ratio(last_sys_response, response) > 80 and \
          beam_size < 5:
        # print('------ re-generate sys response attempt {} ---------'.format(
        #   beam_size-1
        # ))
        # print('-> last response: {}'.format(last_sys_response))

        response, human_eval_response = self._response(
          agent_state, beam_size=beam_size
        )
        beam_size += 1

        # print('-> new response: {}'.format(response))

    return response, human_eval_response

  def _response(self, dialog_history, beam_size=1):
    # get dialog history based on max_turn
    dialog_history = dialog_history[-2 * self.max_turn - 1:]

    # prepare dialog history string
    d_history = []
    for i, utter in enumerate(dialog_history):
      if i % 2 == 0:
        d_history.append('User: {}'.format(utter))
      else:
        d_history.append('System: {}'.format(utter))
    dialog_history_str = ' '.join(d_history)

    #
    # generate belief prediction and query db
    #
    num_beams = 1  # always start with beam_size=1 for belief generation
    while True and num_beams < 5:
      # belief state prediction
      belief_state_eos = 'Ġ<EOB>Ġ'
      belief_state_str = self.model_step(
        dialog_history_str + ' Ġ<SOB>Ġ', stop_token=belief_state_eos,
        num_beams=num_beams
      )
      belief_state_str = post_process_constraints(belief_state_str)
      # print('belief_state_str', belief_state_str)

      #
      # retrieve db state and query db
      #
      try:
        db_state = get_db_state(belief_state_str, dataset='multiwoz')
        if not db_state:
          db_state = 'EMPTY'
        db_state_str = '{} DB: {} Ġ<EOKB>Ġ'.format(
          belief_state_str.replace('<EOB>', 'Ġ<EOB>Ġ').replace('<SOB>', 'Ġ<SOB>Ġ'),
          db_state
        )
        break
      except Exception:
        num_beams += 1
        traceback.print_exc()
        # print('error belief_state_str:', belief_state_str)
        # print('---> re-generate belief prediction attempt {}.'.format(
        #   num_beams
        # ))
        continue

    #
    # predict response
    #
    response_eos = 'Ġ<EOS>Ġ'
    # use different start beam_size to generate different responses
    num_beams = beam_size
    while True and num_beams < beam_size + 5:
      try:
        response_str = self.model_step(db_state_str, response_eos,
                                       num_beams=num_beams)
        # print('response_str', response_str)

        # final response
        response, human_eval_response = get_final_response(
          response_str, dataset='multiwoz'
        )
        # print('response', response)

        # check remaining place holders, continue if there are still
        # remaining placeholders.
        response_placeholders = re.findall(r'\[.*?\]', response)
        human_eval_placeholders = re.findall(r'\[.*?\]', human_eval_response)
        if self.user_simulator:
          remaining_place_holders = response_placeholders
        else:
          remaining_place_holders = human_eval_placeholders
        if len(remaining_place_holders) > 0:
          if num_beams == beam_size + 4:
            # replace remaining placeholders with unknown
            for p in re.findall(r'\[.*?\]', response):
              response = response.replace(p, 'not listed')
            for p in re.findall(r'\[.*?\]', human_eval_response):
              human_eval_response = human_eval_response.replace(p, 'not listed')
            break
          # print('--> found remaining placeholders, continue')
          num_beams += 1
          continue

        break
      except Exception:
        traceback.print_exc()
        num_beams += 1
        # print('error response_str:', response_str)
        # print('---> re-generate response prediction attempt {}.'.format(
        #   num_beams - beam_size
        # ))
        continue

    # print('usr:', dialog_history[-1])
    # print('sys:', response)
    # if response != human_eval_response:
    #   print('sys_human_evl:', human_eval_response)
    # print('belief state: {}'.format(
    #   belief_state_str.replace(dialog_history_str, '').strip()
    # ))
    # print('db: {}'.format(get_db_state(belief_state_str, dataset='multiwoz')))
    # print('response template: {}'.format(
    #   response_str.split('<EOKB>')[1].strip()
    # ))
    # print('')

    return response, human_eval_response

  def model_step(self, input_str, stop_token, num_beams):
    # suppress transformers tokenization warning
    import logging
    from transformers import logger
    logger.setLevel(logging.ERROR)

    if not torch.cuda.is_available():
      module = self.model.nets
    else:
      module = self.model.nets.module

    # make tokenization compatible with transformer==2.9.0
    input_ids = torch.LongTensor([module.tokenizer.convert_tokens_to_ids(
      module.tokenizer.tokenize(input_str, return_tensors="pt")
    )])
    attention_mask = torch.LongTensor([[1] * len(input_ids)])
    inputs = {
      'input_ids': input_ids,
      'attention_mask': attention_mask
    }

    if not torch.cuda.is_available():
      input_ids = inputs['input_ids']
      attention_mask = inputs['attention_mask']
    else:
      input_ids = inputs['input_ids'].cuda()
      attention_mask = inputs['attention_mask'].cuda()

    output_sequences = module.gpt2.generate(
      input_ids=input_ids,
      attention_mask=attention_mask,
      max_length=1024,
      # stop at stop_token
      eos_token_id=module.tokenizer.convert_tokens_to_ids([stop_token])[0],
      num_return_sequences=1,
      num_beams=num_beams,
      do_sample=False
    )
    output_tokens = module.tokenizer.convert_ids_to_tokens(
      output_sequences.tolist()[0]
    )

    if output_tokens[-1] != stop_token:
      output_tokens.append(stop_token)

    rtn = ''.join(output_tokens).replace('Ġ', ' ').strip()

    return rtn


if __name__ == '__main__':
  s = Soloist()
  print(s.response("I want to find a cheap restaurant"))
  print(s.response("ok, what is the address ?"))
